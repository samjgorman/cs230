# -*- coding: utf-8 -*-
"""Detectron2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sNDyeoIpOvgqke4_3o3MYAc57Li0bgRM

# Detectron2 Beginner's Tutorial

<img src="https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png" width="500">

Welcome to detectron2! This is the official colab tutorial of detectron2. Here, we will go through some basics usage of detectron2, including the following:
* Run inference on images or videos, with an existing detectron2 model
* Train a detectron2 model on a new dataset

You can make a copy of this tutorial by "File -> Open in playground mode" and play with it yourself. __DO NOT__ request access to this tutorial.

# Install detectron2
"""

# install dependencies: (use cu101 because colab has CUDA 10.1)
!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html 
!pip install cython pyyaml==5.1
!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
import torch, torchvision
print(torch.__version__, torch.cuda.is_available())
!gcc --version
# opencv is pre-installed on colab

# install detectron2:
!pip install detectron2==0.1.3 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html

# You may need to restart your runtime prior to this, to let your installation take effect
# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import cv2
import random
from google.colab.patches import cv2_imshow

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog

"""# Run a pre-trained detectron2 model

## mount drive
"""

# images = '/content/drive/My Drive/CS230 data/labeleddata/images'
images = '/content/drive/My Drive/CS230 data/preprocessed'

annotations = '/content/drive/My Drive/CS230 data/labeleddata/annotations'
from google.colab import drive
from google.colab import files
drive.mount('/content/drive')

# !cd '/content/drive/My Drive/CS230 data/labeleddata/'
# !mkdir test_labels_detectron train_labels_detectron

"""read random image

# just trying something out
!cd '/content/drive/My Drive/CS230 data'
!mkdir detectron
!cd detectron
!wget https://storage.cloud.google.com/demo-frames-tolabel/math_cs_video3_frame2680.jpg -O input.jpg
im = cv2.imread("input.jpg")
cv2_imshow(im)
"""

# !wget http://images.cocodataset.org/val2017/000000439715.jpg -O input.jpg
im = cv2.imread("/content/drive/My Drive/CS230 data/labeleddata/images/world_video0_frame2080.jpg")
cv2_imshow(im)

"""Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image."""

cfg = get_cfg()
# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor = DefaultPredictor(cfg)
outputs = predictor(im)

# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification
outputs["instances"].pred_classes
outputs["instances"].pred_boxes

# We can use `Visualizer` to draw the predictions on the image.
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(v.get_image()[:, :, ::-1])

"""# Train on a custom dataset"""

# constant: dataset_num
# change this when you want to register another dataset 

dataset_num = "6"

# CONSTANTS

dataset_num = "5"
dataset_name = "titles" + dataset_num + "_train"

PATH_TO_IMAGES = "/content/drive/My Drive/CS230 data/labeleddata/images/"

"""In this section, we show how to train an existing detectron2 model on a custom dataset in a new format.


We'll train a segmentation model from an existing model pre-trained on COCO dataset, available in detectron2's model zoo.

Note that COCO dataset does not have the "title" category. We'll be able to recognize this new class in a few minutes.

## Prepare the dataset

Register the dataset to detectron2, following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).
Here, the dataset is in its custom format, therefore we write a function to parse it and prepare it into detectron2's standard format. See the tutorial for more details.
"""

# if your dataset is in COCO format, this cell can be replaced by the following three lines:
# from detectron2.data.datasets import register_coco_instances
# register_coco_instances("my_dataset_train", {}, "json_annotation_train.json", "path/to/image/dir")
# register_coco_instances("my_dataset_val", {}, "json_annotation_val.json", "path/to/image/dir")

import os
import numpy as np
import json
from detectron2.structures import BoxMode

def get_title_dicts(img_dir):
  # json_file = os.path.join(img_dir, "../dataset_labels.json")
  json_file = img_dir + "../dataset_labels.json"
  with open(json_file) as f:
    imgs_anns = json.load(f)
    # print(imgs_anns)
    dataset_dicts = []

    for idx, img_data in enumerate(imgs_anns):
      # print("img_data", img_data)
      
      # for idx, v in enumerate(img_data.values()):
        # try:
      record = {}
      
      # filename = os.path.join(img_dir, v["External ID"])
      filename = img_dir + img_data["External ID"]
      # height, width = cv2.imread(filename).shape[:2]
      if img_data["Label"] == {}:
        # no label
        continue

  
      record["file_name"] = filename
      record["image_id"] = idx
      record["height"] = 288 #height
      record["width"] = 512 #width
    
      # annos = v["regions"]
      objs = []
      # for _, anno in annos.items():
      #     assert not anno["region_attributes"]
      #     anno = anno["shape_attributes"]
      #     px = anno["all_points_x"]
      #     py = anno["all_points_y"]
      #     poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]
      #     poly = [p for x in poly for p in x]
      for title in img_data["Label"]["objects"]:
        bbox = list(title["bbox"].values())[0:4]
        y, x, h, w = bbox
        obj = {
            "bbox": [x, y, w, h],
            "bbox_mode": BoxMode.XYWH_ABS,
            "segmentation": [[x, y, x+w, y, x+w, y+h, x, y+h]],
            "category_id": 0,
            "iscrowd": 0
        }
        objs.append(obj)
      
      record["annotations"] = objs
      dataset_dicts.append(record)
    # except:
    #   continue;
          
    return dataset_dicts





from detectron2.data import DatasetCatalog, MetadataCatalog
for d in ["train"]: #, "val"]:
    DatasetCatalog.register("titles" + dataset_num + "_" + d, lambda d=d: get_title_dicts("/content/drive/My Drive/CS230 data/labeleddata/images/"))
    MetadataCatalog.get("titles" + dataset_num + "_" + d).set(thing_classes=["title"])
titles_metadata = MetadataCatalog.get(dataset_name)

"""To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:"""

dataset_dicts = get_title_dicts("/content/drive/My Drive/CS230 data/labeleddata/images/")
for d in random.sample(dataset_dicts, 3):
    img = cv2.imread(d["file_name"])

    visualizer = Visualizer(img[:, :, ::-1], metadata=titles_metadata, scale=1)
    # visualizer = Visualizer(img, metadata=titles_metadata, scale=1)

    vis = visualizer.draw_dataset_dict(d)
    cv2_imshow(vis.get_image()[:, :, ::-1])
    # cv2_imshow(vis.get_image())

"""## Train!

Now, let's fine-tune a coco-pretrained R50-FPN Mask R-CNN model on the titles dataset. It takes ~6 minutes to train 300 iterations on Colab's K80 GPU, or ~2 minutes on a P100 GPU.
"""

from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = (dataset_name,)
cfg.DATASETS.TEST = ()
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")  # Let training initialize from model zoo
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.0005 # 0.00025  # pick a good LR
cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (titles)

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg) 
trainer.resume_or_load(resume=False)
trainer.train()

# Commented out IPython magic to ensure Python compatibility.
# Look at training curves in tensorboard:
# %load_ext tensorboard
# %tensorboard --logdir output

"""## Inference & evaluation using the trained model

NOTE TO SELF:
it will only go through whatever is in the dataset_dicts that gets returned from the get_dicts function. So it actually doesn't really matter if all the images are in the same directory or not. 


Now, let's run inference with the trained model on the titles validation dataset. First, let's create a predictor using the model we just trained:
"""

# PREPARE THE VALIDATION SET
"""
def get_val_set_dicts(img_dir):
  # json_file = os.path.join(img_dir, "../dataset_labels.json")
  json_file = img_dir + "../val_set.json"
  with open(json_file) as f:
    imgs_anns = json.load(f)
    # print(imgs_anns)
    dataset_dicts = []

    for idx, img_data in enumerate(imgs_anns):
      # print("img_data", img_data)
      
      # for idx, v in enumerate(img_data.values()):
        # try:
      record = {}
      
      # filename = os.path.join(img_dir, v["External ID"])
      filename = img_dir + img_data["External ID"]
      # height, width = cv2.imread(filename).shape[:2]
      

  
      record["file_name"] = filename
      record["image_id"] = idx
      record["height"] = 288 #height
      record["width"] = 512 #width
    
      # annos = v["regions"]
      objs = []
      # for _, anno in annos.items():
      #     assert not anno["region_attributes"]
      #     anno = anno["shape_attributes"]
      #     px = anno["all_points_x"]
      #     py = anno["all_points_y"]
      #     poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]
      #     poly = [p for x in poly for p in x]
      if img_data["Label"] != {}:
        # there is a label
        for title in img_data["Label"]["objects"]:
          bbox = list(title["bbox"].values())[0:4]
          y, x, h, w = bbox
          obj = {
              "bbox": [x, y, w, h],
              "bbox_mode": BoxMode.XYWH_ABS,
              "segmentation": [[x, y, x+w, y, x+w, y+h, x, y+h]],
              "category_id": 0,
              "iscrowd": 0
          }
          objs.append(obj)
      
      record["annotations"] = objs
      dataset_dicts.append(record)
    # except:
    #   continue;
          
    return dataset_dicts
"""

import re


def get_val_set_dicts(path):
  json_file = path + "val_set_5_24.json"
  with open(json_file) as f:
    imgs_anns = json.load(f)
    # print(imgs_anns)
    dataset_dicts = []

    for idx, img_data in enumerate(imgs_anns):
      # print("img_data", img_data)
      
      # for idx, v in enumerate(img_data.values()):
        # try:
      record = {}


      
      # filename = os.path.join(img_dir, v["External ID"])
     

      url = img_data["Labeled Data"]
      filename = re.search(r"\b/(\w)+\.jpg", url).group()


  
      record["file_name"] = path + "all_images" + filename
      # record["url"] = filename
      record["image_id"] = idx
      record["height"] = 288 #height
      record["width"] = 512 #width
    
      # annos = v["regions"]
      objs = []
      # for _, anno in annos.items():
      #     assert not anno["region_attributes"]
      #     anno = anno["shape_attributes"]
      #     px = anno["all_points_x"]
      #     py = anno["all_points_y"]
      #     poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]
      #     poly = [p for x in poly for p in x]
      if img_data["Label"] != {}:
        # there is a label
        for title in img_data["Label"]["objects"]:
          bbox = list(title["bbox"].values())[0:4]
          y, x, h, w = bbox
          obj = {
              "bbox": [x, y, w, h],
              "bbox_mode": BoxMode.XYWH_ABS,
              "segmentation": [[x, y, x+w, y, x+w, y+h, x, y+h]],
              "category_id": 0,
              "iscrowd": 0
          }
          objs.append(obj)
      
      record["annotations"] = objs
      dataset_dicts.append(record)
    # except:
    #   continue;
          
    return dataset_dicts


path = "/content/drive/My Drive/CS230 data/"


test_dataset_num = "7"
val_set_name = "titles" + test_dataset_num + "_val"

# register
DatasetCatalog.register(val_set_name, lambda: get_val_set_dicts(path))
MetadataCatalog.get(val_set_name).set(thing_classes=["title"])

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model
cfg.DATASETS.TEST = (val_set_name, ) # change to testing dataset
predictor = DefaultPredictor(cfg)

"""Then, we randomly select several samples to visualize the prediction results."""

# import urllib
# from skimage import io
# from urllib.request import urlopen

from detectron2.utils.visualizer import ColorMode
dataset_dicts = get_val_set_dicts(path) # change to test path

for d in random.sample(dataset_dicts, 20):  

    try:

      im = cv2.imread(d["file_name"])
      # im = io.imread(d["file_name"])
      # response = urlopen('https://storage.cloud.google.com/demo-frames-tolabel/math_cs_video3_frame2680.jpg')

      # final_url = response.geturl()
      # print(final_url)
      # im = io.imread(d["file_name"])
      # io.imshow(im)
      # io.show()
      # print(im)

      # url_response = urlopen('https://storage.cloud.google.com/demo-frames-tolabel/math_cs_video3_frame2680.jpg')
      # img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)
      # im = cv2.imdecode(img_array, -1)
    
      # im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)
      # print(im)
      # cv2_imshow(im)
      # cv2_imshow(im)
      outputs = predictor(im)
      v = Visualizer(im[:, :, ::-1],
                    metadata=titles_metadata, 
                    scale=1, 
                    instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels
      )
      v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
      cv2_imshow(v.get_image()[:, :, ::-1])
    except:
      continue

"""We can also evaluate its performance using AP metric implemented in COCO API.
This gives an AP of ~70%. Not bad!
"""

from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data import build_detection_test_loader
evaluator = COCOEvaluator(val_set_name, cfg, False, output_dir=path + "/detectron_output" + test_dataset_num + "/")

val_loader = build_detection_test_loader(cfg, val_set_name)
inference_on_dataset(trainer.model, val_loader, evaluator)
# another equivalent way is to use trainer.test